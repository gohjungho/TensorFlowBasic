{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 지도학습 \n",
    "\n",
    "import numpy as np # 계산 \n",
    "import matplotlib.pyplot as plt # 그림(차트, 플롯)\n",
    "import pandas as pd # 분석 \n",
    "from sklearn import metrics # 모델 성능 평가 \n",
    "\n",
    "# 데이터셋의 열에 이름 할당 \n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class'] \n",
    "\n",
    "# 데이터를 판다스 df에 저장, 경로는 수정 후 진행\n",
    "dataset = pd.read_csv('data/iris.data', names = names)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e9578c85c8e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 특성 스케일링. 평균이 0, 표준편차가 1이 되도록 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 훈련 데이터를 스케일링 처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 검증 데이터를 스케일링 처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[0mCopy\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m         \"\"\"\n\u001b[1;32m--> 788\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# 데이터 전처리, 훈련 및 검증 데이터셋으로 분리하기 \n",
    "\n",
    "# 모든 행을 사용하지만 열은 뒤에서 하나 뺀 값을 가져온다. \n",
    "X = dataset.iloc[:, :-1].values\n",
    "# 모든 행을 사용하지만 열은 앞에서 5번째값만 가져온다. \n",
    "y = dataset.iloc[:, 4].values \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "# 훈련과 검증 데이터셋으로 분리, 검증에는 20% 할당 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler() # 특성 스케일링. 평균이 0, 표준편차가 1이 되도록 변환 \n",
    "X_train = s.transform(X_train) # 훈련 데이터를 스케일링 처리 \n",
    "X_test = s.transform(X_test) # 검증 데이터를 스케일링 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성 및 훈련 \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors = 50) # K = 50인 K-최근접 이웃 모델 생성 \n",
    "knn.fit(X_train, y_train) # 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 모델 정확도 \n",
    "from sklearn.metrics import accuracy_score \n",
    "y_pred = knn.predict(X_test) \n",
    "print(\"정확도: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 0.9666666666666667 으로 최적의 k는 5 입니다.\n"
     ]
    }
   ],
   "source": [
    "# 최적의 K 찾기. k값에 따라 값이 달라진다. \n",
    "k = 10 \n",
    "acc_array = np.zeros(k)\n",
    "for k in np.arange(1, k+1, 1): # K는 1~10의 값을 취함 \n",
    "    # for 문을 반복하면서 K값 변경 \n",
    "    classifier = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred) \n",
    "    acc_array[k-1] = acc \n",
    "    \n",
    "max_acc = np.amax(acc_array)\n",
    "acc_list = list(acc_array)\n",
    "k = acc_list.index(max_acc)\n",
    "print(\"정확도\", max_acc, \"으로 최적의 k는\", k+1, \"입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서포트 벡터 머신 \n",
    "# 데이터(특히 텍스트) 분류에 많이 사용한다. \n",
    "# 분류를 위한 기준선을 정의하는 모델 \n",
    "\n",
    "# 결정 경계 위치 설정하기. 데이터가 분류된 클래스에서 최대한 멀리 떨어져 있을 때 성능이 가장 좋다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection \n",
    "\n",
    "import tensorflow as tf\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터로 훈련, 검증 데이터셋으로 분리 \n",
    "iris = datasets.load_iris() # 데이터 가져오기 \n",
    "# 사이킷런의 model_selection의 train_test_split 메서드를 활용해 훈련셋과 검증셋으로 분리 \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.988889\n"
     ]
    }
   ],
   "source": [
    "# SVM 모델 생성 및 훈련. 검증 데이터를 이요한 예측 수행 \n",
    "\n",
    "# SVM 모델에 대한 정확도 \n",
    "svm = svm.SVC(kernel = 'linear', C = 1.0, gamma = 0.5)\n",
    "svm.fit(X_train, y_train) # 훈련 데이터를 사용하여 SVM 분류기를 훈련 \n",
    "predictions = svm.predict(X_test) # 훈련된 모델을 사용하여 검증 데이터에서 예측 \n",
    "score = metrics.accuracy_score(y_test, predictions) \n",
    "print('정확도: {0:f}'.format(score)) # 검증 데이터 (예측) 정확도 측정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 트리 \n",
    "# 이상치가 많은 값으로 구성된 데이터셋을 다룰 때 효과적이다. \n",
    "# 시각적으로 표현되기 때문에 매우 유용하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지니 계수 \n",
    "# 불순도를 측정하는 지표. 데이터의 통계적 분산 정도를 정량화해서 표현한 값 \n",
    "# 지니 계수가 높을수록 데이터가 분산되어 있음을 의미 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived  Pclass  \\\n",
      "PassengerId                     \n",
      "1                   0       3   \n",
      "2                   1       1   \n",
      "3                   1       3   \n",
      "4                   1       1   \n",
      "5                   0       3   \n",
      "\n",
      "                                                          Name     Sex   Age  \\\n",
      "PassengerId                                                                    \n",
      "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
      "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
      "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
      "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
      "5                                     Allen, Mr. William Henry    male  35.0   \n",
      "\n",
      "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
      "PassengerId                                                          \n",
      "1                1      0         A/5 21171   7.2500   NaN        S  \n",
      "2                1      0          PC 17599  71.2833   C85        C  \n",
      "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "4                1      0            113803  53.1000  C123        S  \n",
      "5                0      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# 결정 트리 예제 - 타이타닉 데이터셋 \n",
    "import pandas as pd \n",
    "# 판다스를 이용해 데이터셋 파일을 로드해 저장 \n",
    "df = pd.read_csv('data/titanic/train.csv', index_col = 'PassengerId') \n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석에 필요한 데이터(칼럼)만 추출하여 전처리 \n",
    "df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Survived']] # 승객의 생존 여부를 예측하기 위함 \n",
    "df['Sex'] = df['Sex'].map({'male':0, 'female':1}) # 'sex'값을 0, 1의 정수 값으로 변환 \n",
    "df = df.dropna() # 결측치(빈값) 삭제 \n",
    "X = df.drop('Survived', axis = 1)\n",
    "y = df['Survived'] # 'Survived' 값을 예측 레이블로 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련과 검증 데이터셋으로 분리 \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 트리 모델 생성 \n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련 \n",
    "model.fit(X_train, y_train) # 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8100558659217877"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 예측 \n",
    "y_predict = model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score \n",
    "accuracy_score(y_test, y_predict) # 에측결과 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동 행렬 \n",
    "# 알고리즘 성능 평가에 사용 \n",
    "# True Positive(TP) - 모델(분류기)이 ‘1’이라고 예측했는데 실제 값도 ‘1’인 경우\n",
    "# True Negative(TN) - 모델(분류기)이 ‘0’이라고 예측했는데 실제 값도 ‘0’인 경우\n",
    "# False Positive(FP) - 모델(분류기)이 ‘1’이라고 예측했는데 실제 값은 ‘0’인 경우\n",
    "# False Negative(FN) - 모델(분류기)이 ‘0’이라고 예측했는데 실제 값은 ‘1’인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Not Survival</th>\n",
       "      <th>Predicted Survival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Not Survival</th>\n",
       "      <td>95</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Survival</th>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Not Survival  Predicted Survival\n",
       "True Not Survival                      95                  17\n",
       "True Survival                          17                  50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 혼동 행렬을 이용한 성능 측정 \n",
    "from sklearn.metrics import confusion_matrix \n",
    "pd.DataFrame(\n",
    "            confusion_matrix(y_test, y_predict), \n",
    "            columns = ['Predicted Not Survival', 'Predicted Survival'], \n",
    "            index = ['True Not Survival', 'True Survival']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀와 선형 회귀 \n",
    "# 회귀란 변수가 두 개 주어졌을 때 한 변수에서 다른 변수를 예측하거나 두 변수의 관계를 규명하는데 사용하는 방법\n",
    "\n",
    "# 독립 변수(예측 변수): 영향을 미칠 것으로 예상되는 변수 \n",
    "# 종속 변수(기준 변수): 영향을 받을 것으로 예상되는 변수 \n",
    "# 이때 두 변수 간 관계에서 독립 변수와 종속 변수의 설정은 논리적인 타당성이 있어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀 \n",
    "# 주어진 데이터에 대한 분류하고자 할때 사용한다. \n",
    "# 주어진 데이터에 대한 확신이 없거나 결과에 대해 확신이 없을 때, \n",
    "# 향후 추가적으로 데이터를 수집하여 모델을 훈련시킬 수 있는 환경에서 유용하다. \n",
    "# 개별 관측치들이 어느 집단에 분류될 수 있는지 분석하고 이를 예측하는 모형을 개발하는데 사용되는 \n",
    "# 통계 기법으로 일반적인 회귀분석과는 차이가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 호출 및 데이터 준비 \n",
    "from sklearn.datasets import load_digits \n",
    "digits = load_digits() # 숫자 데이터셋은 사이킷런에서 제공 \n",
    "\n",
    "print(\"Image Data Shape\", digits.data.shape) # digits 데이터셋의 형태 \n",
    "print(\"Label Data Shape\", digits.target.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
